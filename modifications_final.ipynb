{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV-703, Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "import pandas as pd\n",
    "from models_to_finetune import deit_small_patch16_224, deit_base_patch16_224\n",
    "from datasets import CUBDataset, DOGDataset, FOODDataset\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CenterCrop\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "#RandomHorizontal flip\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "#RandomRotation\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label smoothing definition\n",
    "class LabelSmoothingLoss(torch.nn.Module): \n",
    "    def __init__(self, classes=5, smoothing=0.0, dim=-1): \n",
    "        super(LabelSmoothingLoss, self).__init__() \n",
    "        self.confidence = 1.0 - smoothing \n",
    "        self.smoothing = smoothing \n",
    "        self.cls = classes \n",
    "        self.dim = dim \n",
    "    def forward(self, pred, target): \n",
    "        pred = pred.log_softmax(dim=self.dim) \n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred) \n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1)) \n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) \n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "#To use: criterion = LabelSmoothingLoss()\n",
    "\n",
    "\n",
    "#Focal loss definition\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = torch.nn.CrossEntropyLoss()(inputs, targets)\n",
    "\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "#To use: criterion = FocalLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50 + DeiT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnsembleModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(MyModel,self).__init__()\n",
    "\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        resnet_features = resnet.fc.in_features\n",
    "\n",
    "        self.backbone1 = torch.nn.Sequential(*(list(resnet.children())[:-1]))\n",
    "\n",
    "        transformer = deit_small_patch16_224(pretrained=True, use_top_n_heads=6,use_patch_outputs=False)\n",
    "        transformer_features = transformer.head.in_features\n",
    "\n",
    "        self.backbone2 = transformer\n",
    "\n",
    "        for name,param in resnet.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for name,param in transformer.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.resnet_mlp = torch.nn.Linear(in_features=resnet_features, out_features=1024)\n",
    "        self.transformer_mlp = torch.nn.Linear(in_features=1000, out_features=1024)\n",
    "        \n",
    "        #change no.of classes here\n",
    "        self.common = torch.nn.Linear(in_features=1024, out_features=320)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        resnet_out1 = self.backbone1(x)\n",
    "\n",
    "        transformer_out1 = self.backbone2(x,fine_tune=True)\n",
    "\n",
    "        resnet_out2 = self.resnet_mlp(resnet_out1.reshape(resnet_out1.shape[0],-1))\n",
    "\n",
    "        transformer_out2 = self.transformer_mlp(transformer_out1)\n",
    "\n",
    "        outt = self.common(0.5*resnet_out2+0.5*transformer_out2)\n",
    "\n",
    "        return out\n",
    "    \n",
    "#To use: model = MyEnsembleModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUB-200-2011 (Birds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUBDataset(torchvision.datasets.ImageFolder):\n",
    "    \"\"\"\n",
    "    Dataset class for CUB Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_root_path, caption_root_path=None, split=\"train\", *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_root_path:      path to dir containing images and lists folders\n",
    "            caption_root_path:    path to dir containing captions\n",
    "            split:          train / test\n",
    "            *args:\n",
    "            **kwargs:\n",
    "        \"\"\"\n",
    "        image_info = self.get_file_content(f\"{image_root_path}/images.txt\")\n",
    "        self.image_id_to_name = {y[0]: y[1] for y in [x.strip().split(\" \") for x in image_info]}\n",
    "        split_info = self.get_file_content(f\"{image_root_path}/train_test_split.txt\")\n",
    "        self.split_info = {self.image_id_to_name[y[0]]: y[1] for y in [x.strip().split(\" \") for x in split_info]}\n",
    "        self.split = \"1\" if split == \"train\" else \"0\"\n",
    "        self.caption_root_path = caption_root_path\n",
    "\n",
    "        super(CUBDataset, self).__init__(root=f\"{image_root_path}/images\", is_valid_file=self.is_valid_file,\n",
    "                                         *args, **kwargs)\n",
    "\n",
    "    def is_valid_file(self, x):\n",
    "        return self.split_info[(x[len(self.root) + 1:])] == self.split\n",
    "\n",
    "    @staticmethod\n",
    "    def get_file_content(file_path):\n",
    "        with open(file_path) as fo:\n",
    "            content = fo.readlines()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/apps/local/shared/CV703/datasets/CUB/CUB_200_2011/\"\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "# Write data transform here as per the requirement\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "train_dataset = CUBDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"train\")\n",
    "test_dataset = CUBDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset))\n",
    "print('Number of test samples:', len(test_dataset))\n",
    "\n",
    "\n",
    "# Load in into the torch dataloader to get variable batch size, shuffle \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, drop_last=True, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, drop_last=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(inputs.shape)\n",
    "    print(labels)\n",
    "    print('='*50)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DOGDataset(ImageFolder):\n",
    "    \"\"\"\n",
    "    Dataset class for CUB Dataset\n",
    "    \"\"\"\n",
    "\n",
    " \n",
    "\n",
    "    def __init__(self, image_root_path, caption_root_path=None, split=\"train\", *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_root_path:      path to dir containing images and lists folders\n",
    "            caption_root_path:    path to dir containing captions\n",
    "            split:          train / test\n",
    "            *args:\n",
    "            **kwargs:\n",
    "        \"\"\"\n",
    "        image_info = self.get_file_content(f\"{image_root_path}splits/file_list.mat\")\n",
    "        image_files = [o[0][0] for o in image_info]\n",
    "        \n",
    "        split_info = self.get_file_content(f\"{image_root_path}/splits/{split}_list.mat\")\n",
    "        split_files = [o[0][0] for o in split_info]\n",
    "        self.split_info = {}\n",
    "        if split == 'train' :\n",
    "            for image in image_files:\n",
    "                if image in split_files:\n",
    "                    self.split_info[image] = \"1\"\n",
    "                else:\n",
    "                    self.split_info[image] = \"0\"\n",
    "        elif split== 'test' :\n",
    "            for image in image_files:\n",
    "                if image in split_files:\n",
    "                    self.split_info[image] = \"0\"\n",
    "                else:\n",
    "                    self.split_info[image] = \"1\"\n",
    "                    \n",
    "        self.split = \"1\" if split == \"train\" else \"0\"\n",
    "        self.caption_root_path = caption_root_path\n",
    "\n",
    " \n",
    "\n",
    "        super(DOGDataset, self).__init__(root=f\"{image_root_path}Images\", is_valid_file = self.is_valid_file,\n",
    "                                         *args, **kwargs)\n",
    "\n",
    " \n",
    "\n",
    "    def is_valid_file(self, x):\n",
    "        return self.split_info[(x[len(self.root) + 1:])] == self.split\n",
    "\n",
    " \n",
    "\n",
    "    @staticmethod\n",
    "    def get_file_content(file_path):\n",
    "        content =  scipy.io.loadmat(file_path)\n",
    "        return content['file_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/apps/local/shared/CV703/datasets/dog/\"\n",
    "\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "\n",
    "train_dataset = DOGDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"train\")\n",
    "test_dataset = DOGDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset))\n",
    "print('Number of test samples:', len(test_dataset))\n",
    "\n",
    "# Load in into the torch dataloader to get variable batch size, shuffle \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, drop_last=True, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, drop_last=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    print(inputs.shape)\n",
    "    print(labels)\n",
    "    print('='*50)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUB-200-2011 + Stanford Dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUBDataset(torchvision.datasets.ImageFolder):\n",
    "    \"\"\"\n",
    "    Dataset class for CUB Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_root_path, caption_root_path=None, split=\"train\", *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_root_path:      path to dir containing images and lists folders\n",
    "            caption_root_path:    path to dir containing captions\n",
    "            split:          train / test\n",
    "            *args:\n",
    "            **kwargs:\n",
    "        \"\"\"\n",
    "        image_info = self.get_file_content(f\"{image_root_path}/images.txt\")\n",
    "        self.image_id_to_name = {y[0]: y[1] for y in [x.strip().split(\" \") for x in image_info]}\n",
    "        split_info = self.get_file_content(f\"{image_root_path}/train_test_split.txt\")\n",
    "        self.split_info = {self.image_id_to_name[y[0]]: y[1] for y in [x.strip().split(\" \") for x in split_info]}\n",
    "        self.split = \"1\" if split == \"train\" else \"0\"\n",
    "        self.caption_root_path = caption_root_path\n",
    "\n",
    "        super(CUBDataset, self).__init__(root=f\"{image_root_path}/images\", is_valid_file=self.is_valid_file,\n",
    "                                         *args, **kwargs)\n",
    "\n",
    "    def is_valid_file(self, x):\n",
    "        return self.split_info[(x[len(self.root) + 1:])] == self.split\n",
    "\n",
    "    @staticmethod\n",
    "    def get_file_content(file_path):\n",
    "        with open(file_path) as fo:\n",
    "            content = fo.readlines()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DOGDataset(torchvision.datasets.ImageFolder):\n",
    "    \"\"\"\n",
    "    Dataset class for DOG Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_root_path, caption_root_path=None, split=\"train\", *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_root_path:      path to dir containing images and lists folders\n",
    "            caption_root_path:    path to dir containing captions\n",
    "            split:          train / test\n",
    "            *args:\n",
    "            **kwargs:\n",
    "        \"\"\"\n",
    "        image_info = self.get_file_content(f\"{image_root_path}splits/file_list.mat\")\n",
    "        image_files = [o[0][0] for o in image_info]\n",
    "        \n",
    "        split_info = self.get_file_content(f\"{image_root_path}/splits/{split}_list.mat\")\n",
    "        split_files = [o[0][0] for o in split_info]\n",
    "        self.split_info = {}\n",
    "        if split == 'train' :\n",
    "            for image in image_files:\n",
    "                if image in split_files:\n",
    "                    self.split_info[image] = \"1\"\n",
    "                else:\n",
    "                    self.split_info[image] = \"0\"\n",
    "        elif split== 'test' :\n",
    "            for image in image_files:\n",
    "                if image in split_files:\n",
    "                    self.split_info[image] = \"0\"\n",
    "                else:\n",
    "                    self.split_info[image] = \"1\"\n",
    "                    \n",
    "        self.split = \"1\" if split == \"train\" else \"0\"\n",
    "        self.caption_root_path = caption_root_path\n",
    "\n",
    "        super(DOGDataset, self).__init__(root=f\"{image_root_path}Images\", is_valid_file = self.is_valid_file,\n",
    "                                         *args, **kwargs)\n",
    "        \n",
    "        ## modify class index as we are going to concat to first dataset\n",
    "        self.class_to_idx = {class_: idx+200 for idx, class_ in enumerate(self.class_to_idx)}\n",
    "        \n",
    "    def is_valid_file(self, x):\n",
    "        return self.split_info[(x[len(self.root) + 1:])] == self.split\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        img = Image.open(os.path.join(path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        \n",
    "        ## modify target class index as we are going to concat to first dataset\n",
    "        return img, target + 200\n",
    "\n",
    "    @staticmethod\n",
    "    def get_file_content(file_path):\n",
    "        content =  scipy.io.loadmat(file_path)\n",
    "        return content['file_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUB:\n",
    "data_root_bird = \"/apps/local/shared/CV703/datasets/CUB/CUB_200_2011/\"\n",
    "\n",
    "mean_bird = (0.485, 0.456, 0.406)\n",
    "std_bird = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "# write data transform here as per the requirement\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_bird, std=std_bird)\n",
    "    ])\n",
    "\n",
    "train_dataset_cub = CUBDataset(image_root_path=f\"{data_root_bird}\", transform=data_transform, split=\"train\")\n",
    "test_dataset_cub = CUBDataset(image_root_path=f\"{data_root_bird}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset_cub))\n",
    "print('Number of test samples:', len(test_dataset_cub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dog:\n",
    "mean_dog = (0.485, 0.456, 0.406)\n",
    "std_dog = (0.229, 0.224, 0.225)\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_dog, std=std_dog)\n",
    "    ])\n",
    "\n",
    "\n",
    "data_root_dog = \"/apps/local/shared/CV703/datasets/dog/\"\n",
    "\n",
    "train_dataset_dog = DOGDataset(image_root_path=f\"{data_root_dog}\", transform=data_transform, split=\"train\")\n",
    "test_dataset_dog = DOGDataset(image_root_path=f\"{data_root_dog}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset_dog))\n",
    "print('Number of test samples:', len(test_dataset_dog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated dataloader for CUB and DOG\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "             torch.utils.data.ConcatDataset([train_dataset_cub, train_dataset_dog]),\n",
    "             batch_size=32, shuffle=True,\n",
    "             num_workers=1, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "             torch.utils.data.ConcatDataset([test_dataset_cub, test_dataset_dog]),\n",
    "             batch_size=32, shuffle=True,\n",
    "             num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset_cub), len(train_dataset_dog), len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset_cub), len(test_dataset_dog), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "    print('image :: ', inputs.shape)\n",
    "    print(targets)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FoodX-251 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = \"local\" # comment out if using ds from the shared folder\n",
    "#ds_type = \"shared\" # comment out if using ds from the local folder\n",
    "\n",
    "\n",
    "if (ds_type == \"local\"):\n",
    "    data_dir = \"/home/dmitry.demidov/Documents/Datasets/FoodX-251\"\n",
    "\n",
    "    split = 'train'\n",
    "    train_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    train_df['path'] = train_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}/{split}_set/', x))\n",
    "\n",
    "    split = 'val'\n",
    "    test_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    test_df['path'] = test_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}/{split}_set/', x))\n",
    "\n",
    "elif (ds_type == \"shared\"):\n",
    "    data_dir = \"/apps/local/shared/CV703/datasets/FoodX/food_dataset\"\n",
    "\n",
    "    split = 'train'\n",
    "    train_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    train_df['path'] = train_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}_set/', x))\n",
    "\n",
    "    split = 'val'\n",
    "    test_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    test_df['path'] = test_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}_set/', x))\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Choose dataset type (local/shared)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 118475\n",
      "Number of test samples: 11994\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FOODDataset(train_df)\n",
    "test_dataset = FOODDataset(test_df)\n",
    "print('Number of train samples:', len(train_dataset))\n",
    "print('Number of test samples:', len(test_dataset))\n",
    "\n",
    "# load in into the torch dataloader to get variable batch size, shuffle \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, drop_last=True, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, drop_last=False, shuffle=True) # Not enough memory for more than 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118475 11994\n",
      "462 375\n",
      "torch.Size([256, 3, 224, 224])\n",
      "tensor([100,  60,  87, 152,  25,  95, 179, 210, 187, 165,  65,  69, 178,   4,\n",
      "        218, 207, 128,  55, 250,  94, 246,  44,  64, 146,  85,  13,  17, 215,\n",
      "         81,   9,  39,  91,  28,  17, 148, 113, 232, 151, 128, 116,  58, 166,\n",
      "        176,  94, 247,  79, 157, 129,  25, 147,  39, 237,   7, 113,  21, 155,\n",
      "        230, 168,  70, 190, 214,  21, 160,  35, 150, 106, 124, 194, 152, 228,\n",
      "        136, 198,  94,  30, 214, 249, 190,  31,  98, 221,  19, 115, 139,  74,\n",
      "        142,  13, 138, 177, 178,  37, 156,  74, 122, 105,   6, 132, 205,  11,\n",
      "        212, 245,  89, 217,  18, 212, 226,   3,  74,  68, 123, 171, 181, 152,\n",
      "        249,  58, 143, 247, 202, 113,  39, 243,  55,  40, 147,  88,  12,  38,\n",
      "        250,  30, 220,  89,  50, 146, 107, 139, 163, 134,  42,  37,  93, 123,\n",
      "         13,  39, 148,  95, 197,  70,   0,  56,   8,  84, 201, 131,  98, 161,\n",
      "          0, 196, 132, 244, 227, 143, 172,  35, 176, 133, 142, 241, 163,  94,\n",
      "         99, 228,  93, 198, 189,  42, 151, 122, 150,   0,  79, 220, 145, 117,\n",
      "        240, 235, 171, 164, 111,  59, 250, 142,  68,  64, 209,   4,  55, 118,\n",
      "         89, 123, 177, 102, 144, 108,   1, 116,  25,  20, 193, 169, 121, 238,\n",
      "         84,  24, 221, 151,  79, 135, 160, 203,  19,  37,  35,  85,  32,  22,\n",
      "         95,   2, 133, 150, 146, 235, 133, 112,  45, 221, 202,  71, 133, 226,\n",
      "        232, 196, 246, 231,  26, 128,  97, 136, 177,  50,   6, 140,  63,  56,\n",
      "        123, 110,  37, 102])\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics about the dataset:\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "\n",
    "print(len(train_loader), len(test_loader))\n",
    "\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(inputs.shape)\n",
    "    print(labels)\n",
    "    print('='*50)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training + Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test funcs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, criterion, optimizer, train_dataset, train_loader, epoch):\n",
    "    print('Training....')\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    acc=0.0\n",
    "\n",
    "    with tqdm(train_loader) as p_bar:\n",
    "        for samples, targets in p_bar:\n",
    "            samples = samples.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            #For ResNet50+DeiT ensemble model, change: model(samples)\n",
    "            outputs = model(samples, fine_tune=True) \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss_value = loss.item()\n",
    "            if not math.isfinite(loss_value):\n",
    "                print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "                sys.exit(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += outputs.shape[0] * loss.item()\n",
    "\n",
    "            acc+=torch.sum(outputs.argmax(dim=-1) == targets).item()\n",
    "\n",
    "    loss_print = epoch_loss / len(train_dataset)\n",
    "    acc_print = acc/len(train_dataset)\n",
    "    epoch_print = epoch+1\n",
    "\n",
    "    print(\"Epoch:\", epoch_print, \"|\", \"Loss:\", loss_print)\n",
    "    print(\"Train Accuracy:{0:.3%}\".format(acc_print))\n",
    "\n",
    "    f = open(\"./models/statistics.txt\", \"a\")\n",
    "    text_train = \"Epoch: \" + str(epoch_print) + \", \" + \"Train Loss: \" + str(loss_print) + \", \" + \"Train Accuracy: \" + str(acc_print) + \"\\n\"\n",
    "    f.write(text_train)\n",
    "    f.close()\n",
    "    \n",
    "    torch.save({'state_dict': model.state_dict()}, './models/model_{0}ep_{1:.2}loss.pt'.format(epoch_print, loss_print))\n",
    "\n",
    "    sent_results(text=text_train)\n",
    "\n",
    "    #del samples\n",
    "    #del targets\n",
    "\n",
    "\n",
    "\n",
    "def test(device, model, criterion, test_dataset, test_loader, model_path = './folder/path.pt', test_only = False):\n",
    "    print('Testing....')\n",
    "\n",
    "    if test_only:\n",
    "        print('Test only!')\n",
    "        state_dict = torch.load(model_path)['state_dict']\n",
    "        model.load_state_dict(state_dict)\n",
    "        model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    acc=0.0\n",
    "\n",
    "    with tqdm(test_loader) as p_bar:\n",
    "        for samples, targets in p_bar:\n",
    "            samples = samples.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            #For ResNet50+DeiT ensemble model, change: model(samples)\n",
    "            outputs = model(samples, fine_tune=False)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            epoch_loss += outputs.shape[0] * loss.item()\n",
    "\n",
    "            acc+=torch.sum(outputs.argmax(dim=-1) == targets).item()\n",
    "\n",
    "    acc_print = acc/len(test_dataset)\n",
    "    loss_print = epoch_loss / len(test_dataset)\n",
    "\n",
    "    print(\"Test Loss:\", loss_print)\n",
    "    print('Test Accuracy:{0:.3%}'.format(acc_print))\n",
    "\n",
    "    f = open(\"./models/statistics.txt\", \"a\")\n",
    "    text_test = \"Test Loss: \"+ str(loss_print) + \", \" + \"Test Accuracy: \" + str(acc_print) + \"\\n\"\n",
    "    f.write(text_test)\n",
    "    f.close()\n",
    "\n",
    "    sent_results(text=text_test)\n",
    "\n",
    "    #del samples\n",
    "    #del targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sent results to an email (optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Email sent successfully!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import sent_results\n",
    "\n",
    "# Test:\n",
    "sent_results(text='Start')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=6144, out_features=251, bias=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = deit_base_patch16_224(pretrained=True, use_top_n_heads=12,use_patch_outputs=False).cuda()\n",
    "\n",
    "# # Freeze backbone:\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False #True\n",
    "\n",
    "# Add linear classifier on top:\n",
    "model.head = torch.nn.Linear(in_features=model.head.in_features, out_features=classes_number)\n",
    "model.head.apply(model._init_weights)\n",
    "for param in model.head.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [42:13<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 2.602127376442897\n",
      "Train Accuracy:42.697%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:32<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8005262642318614\n",
      "Test Accuracy:55.544%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [16:37<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 1.8689761778200997\n",
      "Train Accuracy:55.924%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7173048071171892\n",
      "Test Accuracy:57.579%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:31<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 1.6101517504764322\n",
      "Train Accuracy:60.988%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.744987237169203\n",
      "Test Accuracy:57.212%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:33<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss: 1.433250160837103\n",
      "Train Accuracy:64.496%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:23<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7027152762405868\n",
      "Test Accuracy:58.463%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:31<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Loss: 1.3008355920247876\n",
      "Train Accuracy:67.339%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:23<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7374989859994618\n",
      "Test Accuracy:58.479%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:30<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Loss: 1.1942556651289087\n",
      "Train Accuracy:69.594%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7816522898267702\n",
      "Test Accuracy:58.354%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:33<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Loss: 1.101364097748157\n",
      "Train Accuracy:71.672%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8265115668898089\n",
      "Test Accuracy:57.395%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:31<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Loss: 1.022652167380422\n",
      "Train Accuracy:73.396%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:23<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7827207471840536\n",
      "Test Accuracy:58.196%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:30<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Loss: 0.9551903280695674\n",
      "Train Accuracy:74.769%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8510074172593245\n",
      "Test Accuracy:57.295%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:40<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 0.8940659377514045\n",
      "Train Accuracy:76.327%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.9153156694181963\n",
      "Test Accuracy:56.962%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:38<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Loss: 0.83789739592263\n",
      "Train Accuracy:77.691%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8830689602103337\n",
      "Test Accuracy:57.829%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:26<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Loss: 0.7839818278544309\n",
      "Train Accuracy:79.065%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.954761978823363\n",
      "Test Accuracy:56.928%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:25<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Loss: 0.7362238494171123\n",
      "Train Accuracy:80.280%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:19<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.970781674519447\n",
      "Test Accuracy:57.520%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:27<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Loss: 0.6952945371950998\n",
      "Train Accuracy:81.304%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.9945529741864811\n",
      "Test Accuracy:56.670%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:31<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Loss: 0.6554656086664307\n",
      "Train Accuracy:82.232%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.008374331771522\n",
      "Test Accuracy:56.895%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:30<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Loss: 0.6191934487446875\n",
      "Train Accuracy:83.117%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.0412181505903275\n",
      "Test Accuracy:56.837%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:35<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Loss: 0.5853451644984397\n",
      "Train Accuracy:84.175%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.0808106353924196\n",
      "Test Accuracy:56.670%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:34<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Loss: 0.5576447219027575\n",
      "Train Accuracy:84.712%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.0983206890256323\n",
      "Test Accuracy:56.978%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:29<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Loss: 0.5209996763938941\n",
      "Train Accuracy:85.834%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:20<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.127342182098676\n",
      "Test Accuracy:56.537%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:25<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Loss: 0.4986789809140658\n",
      "Train Accuracy:86.390%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.144308435334471\n",
      "Test Accuracy:56.286%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:29<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Loss: 0.47348117491091624\n",
      "Train Accuracy:87.008%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.1851335905861293\n",
      "Test Accuracy:56.345%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:32<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Loss: 0.44692795968604104\n",
      "Train Accuracy:87.819%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.18681697593404\n",
      "Test Accuracy:56.495%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:31<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Loss: 0.4275366283705728\n",
      "Train Accuracy:88.329%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.247680110336642\n",
      "Test Accuracy:55.761%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:21<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Loss: 0.4015784313478005\n",
      "Train Accuracy:89.214%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.245173689323961\n",
      "Test Accuracy:56.495%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:22<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Loss: 0.38088590877522494\n",
      "Train Accuracy:89.826%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:20<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3162277274794114\n",
      "Test Accuracy:55.611%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:33<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Loss: 0.3643084753180989\n",
      "Train Accuracy:90.282%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3012560301270706\n",
      "Test Accuracy:56.020%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:38<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Loss: 0.34492365286304466\n",
      "Train Accuracy:90.701%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3399747711271965\n",
      "Test Accuracy:55.786%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:36<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Loss: 0.32351259169183116\n",
      "Train Accuracy:91.526%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.373192827284525\n",
      "Test Accuracy:55.211%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:36<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Loss: 0.31325340758800807\n",
      "Train Accuracy:91.733%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.399460998320154\n",
      "Test Accuracy:55.386%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:36<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Loss: 0.29862291540921454\n",
      "Train Accuracy:92.140%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:23<00:00,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.429801371210075\n",
      "Test Accuracy:55.278%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "#change criterion here\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train (device=device, model=model, criterion=criterion, optimizer=optimizer, \n",
    "        train_dataset=train_dataset, train_loader=train_loader, epoch=epoch)\n",
    "\n",
    "    test (device=device, model=model, criterion=criterion, \n",
    "        test_dataset=test_dataset, test_loader=test_loader, test_only = False) #, model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test only:\n",
    "To check a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing....\n",
      "Test only!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:26<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7027152970530142\n",
      "Test Accuracy:58.463%\n"
     ]
    }
   ],
   "source": [
    "# It is assumed that a model is defined already\n",
    "model_path = './models/model_3ep_1.4loss.pt'\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "test (device=device, model=model, criterion=criterion, \n",
    "    test_dataset=test_dataset, test_loader=test_loader, test_only = True, model_path=model_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1a7d7175cab92eb6dbde117c0cc0b7f8055ecb32409ac704c7d8562db6b2b75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
