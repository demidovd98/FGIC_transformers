{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV-703, Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from models_to_finetune import deit_small_patch16_224, deit_base_patch16_224, resnet50\n",
    "\n",
    "from datasets import CUBDataset, DOGDataset, FOODDataset\n",
    "\n",
    "import sys\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup a dataset\n",
    "Uncomment that one dataset you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUB-200-2011 (Birds): Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 5994\n",
      "Number of test samples: 5794\n"
     ]
    }
   ],
   "source": [
    "data_root = \"/apps/local/shared/CV703/datasets/CUB/CUB_200_2011/\"\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "# Write data transform here as per the requirement\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "train_dataset = CUBDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"train\")\n",
    "test_dataset = CUBDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset))\n",
    "print('Number of test samples:', len(test_dataset))\n",
    "\n",
    "\n",
    "# Load in into the torch dataloader to get variable batch size, shuffle \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, drop_last=True, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, drop_last=False, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187, 182)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "tensor([149, 190, 181, 127,  38,  61, 145, 174,  13, 108, 144,  86,  88, 156,\n",
      "        182,  54, 122, 153,  49, 172,  10,  33, 197, 120,  22, 139,  77,  15,\n",
      "         67, 102,  25, 175])\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(inputs.shape)\n",
    "    print(labels)\n",
    "    print('='*50)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Dogs: Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 12000\n",
      "Number of test samples: 8580\n"
     ]
    }
   ],
   "source": [
    "data_root = \"/apps/local/shared/CV703/datasets/dog/\"\n",
    "\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "\n",
    "train_dataset = DOGDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"train\")\n",
    "test_dataset = DOGDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset))\n",
    "print('Number of test samples:', len(test_dataset))\n",
    "\n",
    "# Load in into the torch dataloader to get variable batch size, shuffle \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, drop_last=True, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, drop_last=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 269)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "tensor([106,  96,  79,  57,  92,  59, 104,  19, 103,  40,   0, 118, 102,  86,\n",
      "         98,  57,  27,  57,  85,  61,  17,  65, 116,  38,  95, 104,  51,   2,\n",
      "         33,  24,  52,   4])\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    print(inputs.shape)\n",
    "    print(labels)\n",
    "    print('='*50)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUB-200-2011 + Stanford Dog: concatenated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 5994\n",
      "Number of test samples: 5794\n"
     ]
    }
   ],
   "source": [
    "# CUB:\n",
    "data_root_bird = \"/apps/local/shared/CV703/datasets/CUB/CUB_200_2011/\"\n",
    "\n",
    "mean_bird = (0.485, 0.456, 0.406)\n",
    "std_bird = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "# write data transform here as per the requirement\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_bird, std=std_bird)\n",
    "    ])\n",
    "\n",
    "train_dataset_cub = CUBDataset(image_root_path=f\"{data_root_bird}\", transform=data_transform, split=\"train\")\n",
    "test_dataset_cub = CUBDataset(image_root_path=f\"{data_root_bird}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset_cub))\n",
    "print('Number of test samples:', len(test_dataset_cub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 12000\n",
      "Number of test samples: 8580\n"
     ]
    }
   ],
   "source": [
    "# Dog:\n",
    "mean_dog = (0.485, 0.456, 0.406)\n",
    "std_dog = (0.229, 0.224, 0.225)\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_dog, std=std_dog)\n",
    "    ])\n",
    "\n",
    "\n",
    "data_root_dog = \"/apps/local/shared/CV703/datasets/dog/\"\n",
    "\n",
    "train_dataset_dog = DOGDataset(image_root_path=f\"{data_root_dog}\", transform=data_transform, split=\"train\")\n",
    "test_dataset_dog = DOGDataset(image_root_path=f\"{data_root_dog}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset_dog))\n",
    "print('Number of test samples:', len(test_dataset_dog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated dataloader for CUB and DOG\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "             torch.utils.data.ConcatDataset([train_dataset_cub, train_dataset_dog]),\n",
    "             batch_size=32, shuffle=True,\n",
    "             num_workers=1, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "             torch.utils.data.ConcatDataset([test_dataset_cub, test_dataset_dog]),\n",
    "             batch_size=32, shuffle=True,\n",
    "             num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5994, 12000, 563)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset_cub), len(train_dataset_dog), len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5794, 8580, 450)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset_cub), len(test_dataset_dog), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image ::  torch.Size([32, 3, 224, 224])\n",
      "tensor([104,  74,   8,  87,  60, 115,  82, 123,  29, 171,  20, 149, 107,  55,\n",
      "         97,  33,  41,  80,  26,  97,  73,   4,  94,   5, 106,  70,  95,  12,\n",
      "         90,   4, 109, 102])\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "    print('image :: ', inputs.shape)\n",
    "    print(targets)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FoodX-251 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = \"local\" # comment out if using ds from the shared folder\n",
    "#ds_type = \"shared\" # comment out if using ds from the local folder\n",
    "\n",
    "\n",
    "if (ds_type == \"local\"):\n",
    "    data_dir = \"/home/dmitry.demidov/Documents/Datasets/FoodX-251\"\n",
    "\n",
    "    split = 'train'\n",
    "    train_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    train_df['path'] = train_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}/{split}_set/', x))\n",
    "\n",
    "    split = 'val'\n",
    "    test_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    test_df['path'] = test_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}/{split}_set/', x))\n",
    "\n",
    "elif (ds_type == \"shared\"):\n",
    "    data_dir = \"/apps/local/shared/CV703/datasets/FoodX/food_dataset\"\n",
    "\n",
    "    split = 'train'\n",
    "    train_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    train_df['path'] = train_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}_set/', x))\n",
    "\n",
    "    split = 'val'\n",
    "    test_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    test_df['path'] = test_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}_set/', x))\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Choose dataset type (local/shared)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 118475\n",
      "Number of test samples: 11994\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FOODDataset(train_df)\n",
    "test_dataset = FOODDataset(test_df)\n",
    "print('Number of train samples:', len(train_dataset))\n",
    "print('Number of test samples:', len(test_dataset))\n",
    "\n",
    "# load in into the torch dataloader to get variable batch size, shuffle \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, drop_last=True, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, drop_last=False, shuffle=True) # Not enough memory for more than 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118475 11994\n",
      "115 12\n",
      "torch.Size([1024, 3, 224, 224])\n",
      "tensor([ 40, 130, 161,  ...,   9,  84,   3])\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics about the dataset:\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "\n",
    "print(len(train_loader), len(test_loader))\n",
    "\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(inputs.shape)\n",
    "    print(labels)\n",
    "    print('='*50)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # we will use only the last class token (produced by the last block) for transfer learning\n",
    "# model = deit_base_patch16_224(pretrained=True, use_top_n_heads=8,use_patch_outputs=False).cuda()\n",
    "\n",
    "# # freeze backbone and add linear classifier on top that\n",
    "# # for param in model.parameters():\n",
    "# #     param.requires_grad = True # False\n",
    "# model.head = torch.nn.Linear(in_features=model.head.in_features, out_features=classes_number)\n",
    "\n",
    "# model.head.apply(model._init_weights)\n",
    "# # for param in model.head.parameters():\n",
    "# #     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.003, betas=(0.5, 0.999))\n",
    "\n",
    "# #model.train()\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training + Evaluation (in parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test funcs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, criterion, optimizer, train_dataset, train_loader, epoch, resnet=False):\n",
    "    print('Training....')\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    acc=0.0\n",
    "\n",
    "    with tqdm(train_loader) as p_bar:\n",
    "        for samples, targets in p_bar:\n",
    "            samples = samples.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            if resnet:\n",
    "                outputs = model(samples)\n",
    "            else:\n",
    "                outputs = model(samples, fine_tune=True)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss_value = loss.item()\n",
    "            if not math.isfinite(loss_value):\n",
    "                print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "                sys.exit(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += outputs.shape[0] * loss.item()\n",
    "\n",
    "            acc+=torch.sum(outputs.argmax(dim=-1) == targets).item()\n",
    "\n",
    "    loss_print = epoch_loss / len(train_dataset)\n",
    "    acc_print = acc/len(train_dataset)\n",
    "    epoch_print = epoch+1\n",
    "\n",
    "    print(\"Epoch:\", epoch_print, \"|\", \"Loss:\", loss_print)\n",
    "    print(\"Train Accuracy:{0:.3%}\".format(acc_print))\n",
    "\n",
    "    f = open(\"./models/statistics.txt\", \"a\")\n",
    "    text_train = \"Epoch: \" + str(epoch_print) + \", \" + \"Train Loss: \" + str(loss_print) + \", \" + \"Train Accuracy: \" + str(acc_print) + \"\\n\"\n",
    "    f.write(text_train)\n",
    "    f.close()\n",
    "    \n",
    "    torch.save({'state_dict': model.state_dict()}, './models/model_{0}ep_{1:.2}loss.pt'.format(epoch_print, loss_print))\n",
    "\n",
    "    sent_results(text=text_train)\n",
    "\n",
    "    #del samples\n",
    "    #del targets\n",
    "\n",
    "\n",
    "\n",
    "def test(device, model, criterion, test_dataset, test_loader, model_path = './folder/path.pt', test_only = False, resnet=False):\n",
    "    print('Testing....')\n",
    "\n",
    "    if test_only:\n",
    "        print('Test only!')\n",
    "        state_dict = torch.load(model_path)['state_dict']\n",
    "        model.load_state_dict(state_dict)\n",
    "        model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    acc=0.0\n",
    "\n",
    "    with tqdm(test_loader) as p_bar:\n",
    "        for samples, targets in p_bar:\n",
    "            samples = samples.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            if resnet:\n",
    "                outputs = model(samples)\n",
    "            else:\n",
    "                outputs = model(samples, fine_tune=False)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            epoch_loss += outputs.shape[0] * loss.item()\n",
    "\n",
    "            acc+=torch.sum(outputs.argmax(dim=-1) == targets).item()\n",
    "\n",
    "    acc_print = acc/len(test_dataset)\n",
    "    loss_print = epoch_loss / len(test_dataset)\n",
    "\n",
    "    print(\"Test Loss:\", loss_print)\n",
    "    print('Test Accuracy:{0:.3%}'.format(acc_print))\n",
    "\n",
    "    f = open(\"./models/statistics.txt\", \"a\")\n",
    "    text_test = \"Test Loss: \"+ str(loss_print) + \", \" + \"Test Accuracy: \" + str(acc_print) + \"\\n\" + \"\\n\"\n",
    "    f.write(text_test)\n",
    "    f.close()\n",
    "\n",
    "    sent_results(text=text_test)\n",
    "\n",
    "    #del samples\n",
    "    #del targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send results to an email (optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Email sent successfully!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import sent_results\n",
    "\n",
    "# Test:\n",
    "sent_results(text='Start')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=9216, out_features=251, bias=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = deit_base_patch16_224(pretrained=True, use_top_n_heads=12,use_patch_outputs=False) #.cuda()\n",
    "#model = deit_small_patch16_224(pretrained=True, use_top_n_heads=8,use_patch_outputs=False).cuda()\n",
    "\n",
    "# # Freeze backbone:\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True # original: False\n",
    "\n",
    "# Add linear classifier on top:\n",
    "model.head = torch.nn.Linear(in_features=model.head.in_features, out_features=classes_number)\n",
    "model.head.apply(model._init_weights)\n",
    "# for param in model.head.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_weights(m):\n",
    "#     if isinstance(m, torch.nn.Linear):\n",
    "#         torch.nn.init.xavier_uniform_(m.weight)\n",
    "#         m.bias.data.fill_(0.01)\n",
    "\n",
    "model = models.resnet50(pretrained=True) #.cuda()\n",
    "model.fc = torch.nn.Linear(in_features=model.fc.in_features, out_features=classes_number, bias=True)\n",
    "#model.fc.apply(init_weights)\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True # original: False\n",
    "\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "import timm\n",
    "\n",
    "model = timm.create_model('inception_resnet_v2', pretrained=True, num_classes=classes_number) #.cuda()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True # original: False\n",
    "\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training + validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train (device=device, model=model, criterion=criterion, optimizer=optimizer, \n",
    "        train_dataset=train_dataset, train_loader=train_loader, epoch=epoch)\n",
    "\n",
    "    test (device=device, model=model, criterion=criterion, \n",
    "        test_dataset=test_dataset, test_loader=test_loader, test_only = False) #, model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train (device=device, model=model, criterion=criterion, optimizer=optimizer, \n",
    "        train_dataset=train_dataset, train_loader=train_loader, epoch=epoch, resnet=True)\n",
    "\n",
    "    test (device=device, model=model, criterion=criterion, \n",
    "        test_dataset=test_dataset, test_loader=test_loader, test_only = False, resnet=True) #, model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid model (Concatenated version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, cnn, deit):\n",
    "\n",
    "        super(MyModel,self).__init__()\n",
    "        \n",
    "        self.cnn = cnn\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False # original: False\n",
    "        \n",
    "        self.cnn.fc = torch.nn.Linear(in_features=self.cnn.fc.in_features, out_features=1024, bias=True)\n",
    "        #model.fc.apply(init_weights)\n",
    "        for param in self.cnn.fc.parameters():\n",
    "            param.requires_grad = True # original: False\n",
    "\n",
    "        #print(self.cnn)\n",
    "\n",
    "        \n",
    "        self.deit = deit\n",
    "        # Freeze backbone:\n",
    "        for param in self.deit.parameters():\n",
    "            param.requires_grad = False # original: False\n",
    "\n",
    "        self.deit.head = torch.nn.Linear(in_features=self.deit.head.in_features, out_features=1024)\n",
    "        self.deit.head.apply(self.deit._init_weights)\n",
    "        # Unfreeze linear classifier on top:\n",
    "        for param in self.deit.head.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        #print(self.deit)\n",
    "\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(in_features=2048, out_features=251)\n",
    "        #self.fc2 = torch.nn.Linear(4096, 251)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.drop1 = torch.nn.Dropout(p=0.1)\n",
    "        #self.drop5 = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "        self.bn1 = torch.nn.BatchNorm1d(num_features=1024)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(num_features=2048)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        cnn_out = self.cnn(x)\n",
    "        #print(cnn_out.shape)\n",
    "        cnn_out = self.bn1(cnn_out)\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        #cnn_out = self.drop1(cnn_out)\n",
    "\n",
    "        deit_out = self.deit(x, fine_tune=False)\n",
    "        #print(deit_out.shape)\n",
    "        deit_out = self.bn1(deit_out)\n",
    "        deit_out = self.relu(deit_out)\n",
    "        #deit_out = self.drop1(deit_out)\n",
    "\n",
    "        concat = torch.cat((cnn_out, deit_out), dim=1)\n",
    "        #concat = self.bn2(concat)\n",
    "        out = self.drop1(concat)\n",
    "        #out = self.relu(out)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        #out = self.drop(out)\n",
    "        #out = self.relu(out)\n",
    "\n",
    "        #out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "deit = deit_base_patch16_224(pretrained=True, use_top_n_heads=12,use_patch_outputs=False)\n",
    "deit.head = torch.nn.Linear(in_features=deit.head.in_features, out_features=classes_number)\n",
    "deit_model_path = './models/7_model_0.0001lr_40ep_256bs_12hs_62.748acc_0.9beta(11hrs)/model_24ep_1.2loss.pt'\n",
    "deit_state_dict = torch.load(deit_model_path)['state_dict']\n",
    "deit.load_state_dict(deit_state_dict)\n",
    "\n",
    "#cnn = timm.create_model('inception_resnet_v2', pretrained=True, num_classes=251)\n",
    "cnn = models.resnet50(pretrained=True) #.cuda()\n",
    "cnn.fc = torch.nn.Linear(in_features=cnn.fc.in_features, out_features=classes_number, bias=True)\n",
    "cnn_model_path = './models/8_Resnet_model_0.0001lr_10ep_256bs_12hs_60.81acc_0.9beta/model_3ep_1.3loss.pt'\n",
    "cnn_state_dict = torch.load(cnn_model_path)['state_dict']\n",
    "cnn.load_state_dict(cnn_state_dict)\n",
    "\n",
    "\n",
    "model = MyModel(deit=deit, cnn=cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Trainig + Validation (in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/115 [00:00<?, ?it/s]/home/dmitry.demidov/.conda/envs/cv703/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "100%|██████████| 115/115 [19:34<00:00, 10.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 1.6328958998593783\n",
      "Train Accuracy:68.364%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:53<00:00,  9.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.826009660556711\n",
      "Test Accuracy:60.747%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [18:59<00:00,  9.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.852160435886065\n",
      "Train Accuracy:78.904%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:47<00:00,  8.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.6772599867309792\n",
      "Test Accuracy:60.722%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:50<00:00,  9.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.687131504268932\n",
      "Train Accuracy:82.240%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:47<00:00,  8.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.5930457393865058\n",
      "Test Accuracy:61.889%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:39<00:00,  9.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss: 0.5700565047222845\n",
      "Train Accuracy:85.121%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:49<00:00,  9.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.5832301511433753\n",
      "Test Accuracy:61.406%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:59<00:00,  9.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Loss: 0.4729790869521853\n",
      "Train Accuracy:87.551%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:47<00:00,  8.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.5794579389593453\n",
      "Test Accuracy:61.064%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:53<00:00,  9.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Loss: 0.38528377239149214\n",
      "Train Accuracy:89.942%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:48<00:00,  9.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.6117299350476453\n",
      "Test Accuracy:60.205%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:47<00:00,  9.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Loss: 0.30871968758964013\n",
      "Train Accuracy:92.231%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:47<00:00,  8.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.6603730330770168\n",
      "Test Accuracy:58.971%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:48<00:00,  9.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Loss: 0.24400462053777497\n",
      "Train Accuracy:94.106%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:49<00:00,  9.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.6738727401649434\n",
      "Test Accuracy:58.921%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:48<00:00,  9.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Loss: 0.18901350869385847\n",
      "Train Accuracy:95.674%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:48<00:00,  9.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7756556890518522\n",
      "Test Accuracy:57.254%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:55<00:00,  9.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 0.14236585904838012\n",
      "Train Accuracy:97.021%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:47<00:00,  8.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7388536969641755\n",
      "Test Accuracy:58.171%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:46<00:00,  9.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Loss: 0.10770389071494926\n",
      "Train Accuracy:97.865%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:48<00:00,  9.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8449679178576956\n",
      "Test Accuracy:56.587%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:41<00:00,  9.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Loss: 0.08061955339472412\n",
      "Train Accuracy:98.476%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:47<00:00,  8.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8806450983991778\n",
      "Test Accuracy:56.236%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:47<00:00,  9.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Loss: 0.059886080952580155\n",
      "Train Accuracy:98.872%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:48<00:00,  9.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.9308918062321083\n",
      "Test Accuracy:56.186%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:49<00:00,  9.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Loss: 0.046388139758942175\n",
      "Train Accuracy:99.055%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:49<00:00,  9.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.9773734502639695\n",
      "Test Accuracy:55.461%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:48<00:00,  9.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Loss: 0.0363893093955142\n",
      "Train Accuracy:99.196%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:47<00:00,  8.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.9268262550913136\n",
      "Test Accuracy:56.978%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:40<00:00,  9.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Loss: 0.030117911753158425\n",
      "Train Accuracy:99.233%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:48<00:00,  9.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.9671115471518832\n",
      "Test Accuracy:56.345%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:46<00:00,  9.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Loss: 0.025163265195268195\n",
      "Train Accuracy:99.263%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:48<00:00,  9.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.003206626124952\n",
      "Test Accuracy:56.161%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:48<00:00,  9.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Loss: 0.021921610448852977\n",
      "Train Accuracy:99.288%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:46<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.0329765194114775\n",
      "Test Accuracy:56.070%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:44<00:00,  9.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Loss: 0.018074160614303458\n",
      "Train Accuracy:99.330%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:48<00:00,  9.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.091519903576094\n",
      "Test Accuracy:54.894%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:45<00:00,  9.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Loss: 0.01631536721629819\n",
      "Train Accuracy:99.304%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:48<00:00,  9.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.0543602220372437\n",
      "Test Accuracy:55.920%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [18:21<00:00,  9.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Loss: 0.01703284069414656\n",
      "Train Accuracy:99.261%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:44<00:00,  8.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.189057146169075\n",
      "Test Accuracy:54.202%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:40<00:00,  9.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Loss: 0.017596712347877054\n",
      "Train Accuracy:99.221%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:49<00:00,  9.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.133411465952232\n",
      "Test Accuracy:55.436%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:39<00:00,  9.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Loss: 0.023583022304968785\n",
      "Train Accuracy:99.099%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:45<00:00,  8.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.2593166538729594\n",
      "Test Accuracy:53.885%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:48<00:00,  9.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Loss: 0.024664284005198305\n",
      "Train Accuracy:99.092%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:47<00:00,  8.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.208838388723832\n",
      "Test Accuracy:54.594%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:46<00:00,  9.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Loss: 0.01588483785068619\n",
      "Train Accuracy:99.276%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:45<00:00,  8.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.277633578892209\n",
      "Test Accuracy:54.361%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:38<00:00,  9.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Loss: 0.012285871173692016\n",
      "Train Accuracy:99.309%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:45<00:00,  8.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.2342845567449126\n",
      "Test Accuracy:55.253%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:35<00:00,  9.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Loss: 0.011149513697820416\n",
      "Train Accuracy:99.299%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:46<00:00,  8.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.9351679451051744\n",
      "Test Accuracy:46.473%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:40<00:00,  9.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Loss: 0.012207813886782116\n",
      "Train Accuracy:99.259%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:43<00:00,  8.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.4493520761581786\n",
      "Test Accuracy:53.027%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:33<00:00,  9.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Loss: 0.03308883852132591\n",
      "Train Accuracy:98.793%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:47<00:00,  8.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3130396968506965\n",
      "Test Accuracy:54.769%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [17:44<00:00,  9.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Loss: 0.039471025243543026\n",
      "Train Accuracy:98.633%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:46<00:00,  8.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.197996882213957\n",
      "Test Accuracy:57.020%\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0003\n",
    "# criterion = FocalLoss().to(device)\n",
    "epochs = 30\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train (device=device, model=model, criterion=criterion, optimizer=optimizer, \n",
    "        train_dataset=train_dataset, train_loader=train_loader, epoch=epoch, resnet=True)\n",
    "\n",
    "    test (device=device, model=model, criterion=criterion, \n",
    "        test_dataset=test_dataset, test_loader=test_loader, test_only = False, resnet=True) #, model_path=model_path)\n",
    "\n",
    "    if ((epoch+1)%3==0):\n",
    "        lr = lr / epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test only (optional):\n",
    "To check a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing....\n",
      "Test only!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:26<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7027152970530142\n",
      "Test Accuracy:58.463%\n"
     ]
    }
   ],
   "source": [
    "# It is assumed that a model is defined already\n",
    "model_path = './models/model_3ep_1.4loss.pt'\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "test (device=device, model=model, criterion=criterion, \n",
    "    test_dataset=test_dataset, test_loader=test_loader, test_only = True, model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = torch.nn.CrossEntropyLoss()(inputs, targets)\n",
    "\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CenterCrop\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "#RandomHorizontal flip\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "#RandomRotation\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label smoothing definition\n",
    "class LabelSmoothingLoss(torch.nn.Module): \n",
    "    def __init__(self, classes=5, smoothing=0.0, dim=-1): \n",
    "        super(LabelSmoothingLoss, self).__init__() \n",
    "        self.confidence = 1.0 - smoothing \n",
    "        self.smoothing = smoothing \n",
    "        self.cls = classes \n",
    "        self.dim = dim \n",
    "    def forward(self, pred, target): \n",
    "        pred = pred.log_softmax(dim=self.dim) \n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred) \n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1)) \n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) \n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "#To use: criterion = LabelSmoothingLoss()\n",
    "\n",
    "\n",
    "#Focal loss definition\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = torch.nn.CrossEntropyLoss()(inputs, targets)\n",
    "\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "#To use: criterion = FocalLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeiT + ResNet-50 (weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnsembleModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(MyModel,self).__init__()\n",
    "\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        resnet_features = resnet.fc.in_features\n",
    "\n",
    "        self.backbone1 = torch.nn.Sequential(*(list(resnet.children())[:-1]))\n",
    "\n",
    "        transformer = deit_small_patch16_224(pretrained=True, use_top_n_heads=6,use_patch_outputs=False)\n",
    "        transformer_features = transformer.head.in_features\n",
    "\n",
    "        self.backbone2 = transformer\n",
    "\n",
    "        for name,param in resnet.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for name,param in transformer.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.resnet_mlp = torch.nn.Linear(in_features=resnet_features, out_features=1024)\n",
    "        self.transformer_mlp = torch.nn.Linear(in_features=1000, out_features=1024)\n",
    "        \n",
    "        #change no.of classes here\n",
    "        self.common = torch.nn.Linear(in_features=1024, out_features=320)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        resnet_out1 = self.backbone1(x)\n",
    "\n",
    "        transformer_out1 = self.backbone2(x,fine_tune=True)\n",
    "\n",
    "        resnet_out2 = self.resnet_mlp(resnet_out1.reshape(resnet_out1.shape[0],-1))\n",
    "\n",
    "        transformer_out2 = self.transformer_mlp(transformer_out1)\n",
    "\n",
    "        outt = self.common(0.5*resnet_out2+0.5*transformer_out2)\n",
    "\n",
    "        return out\n",
    "    \n",
    "#To use: model = MyEnsembleModel()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1a7d7175cab92eb6dbde117c0cc0b7f8055ecb32409ac704c7d8562db6b2b75"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('cv703': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}