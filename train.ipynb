{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV-703, Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from models_to_finetune import deit_small_patch16_224, deit_base_patch16_224\n",
    "\n",
    "from datasets import CUBDataset, DOGDataset, FOODDataset\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# from __future__ import print_function, division\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.optim import lr_scheduler\n",
    "\n",
    "# import torchvision\n",
    "# from torchvision import datasets, models, transforms\n",
    "# import torchvision.transforms as T\n",
    "\n",
    "# from PIL import Image\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# import scipy.io #for dogs dateset\n",
    "\n",
    "# import time\n",
    "# import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup a dataset\n",
    "Uncomment that one dataset you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUB-200-2011 (Birds): Dataset (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/apps/local/shared/CV703/datasets/CUB/CUB_200_2011/\"\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "# Write data transform here as per the requirement\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "train_dataset = CUBDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"train\")\n",
    "test_dataset = CUBDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset))\n",
    "print('Number of test samples:', len(test_dataset))\n",
    "\n",
    "\n",
    "# Load in into the torch dataloader to get variable batch size, shuffle \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, drop_last=True, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, drop_last=False, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(inputs.shape)\n",
    "    print(labels)\n",
    "    print('='*50)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Dogs: Dataset (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/apps/local/shared/CV703/datasets/dog/\"\n",
    "\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "\n",
    "train_dataset = DOGDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"train\")\n",
    "test_dataset = DOGDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset))\n",
    "print('Number of test samples:', len(test_dataset))\n",
    "\n",
    "# Load in into the torch dataloader to get variable batch size, shuffle \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, drop_last=True, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, drop_last=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    print(inputs.shape)\n",
    "    print(labels)\n",
    "    print('='*50)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUB-200-2011 + Stanford Dog: concatenated Dataset (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUB:\n",
    "data_root_bird = \"/apps/local/shared/CV703/datasets/CUB/CUB_200_2011/\"\n",
    "\n",
    "mean_bird = (0.485, 0.456, 0.406)\n",
    "std_bird = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "# write data transform here as per the requirement\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_bird, std=std_bird)\n",
    "    ])\n",
    "\n",
    "train_dataset_cub = CUBDataset(image_root_path=f\"{data_root_bird}\", transform=data_transform, split=\"train\")\n",
    "test_dataset_cub = CUBDataset(image_root_path=f\"{data_root_bird}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset_cub))\n",
    "print('Number of test samples:', len(test_dataset_cub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dog:\n",
    "mean_dog = (0.485, 0.456, 0.406)\n",
    "std_dog = (0.229, 0.224, 0.225)\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_dog, std=std_dog)\n",
    "    ])\n",
    "\n",
    "\n",
    "data_root_dog = \"/apps/local/shared/CV703/datasets/dog/\"\n",
    "\n",
    "# TODO: Start labels counting from 200 (in case of concatenation only)!\n",
    "train_dataset_dog = DOGDataset(image_root_path=f\"{data_root_dog}\", transform=data_transform, split=\"train\")\n",
    "test_dataset_dog = DOGDataset(image_root_path=f\"{data_root_dog}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset_dog))\n",
    "print('Number of test samples:', len(test_dataset_dog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated dataloader for CUB and DOG\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "             torch.utils.data.ConcatDataset([train_dataset_cub, train_dataset_dog]),\n",
    "             batch_size=32, shuffle=True,\n",
    "             num_workers=1, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "             torch.utils.data.ConcatDataset([test_dataset_cub, test_dataset_dog]),\n",
    "             batch_size=32, shuffle=True,\n",
    "             num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset_cub), len(train_dataset_dog), len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset_cub), len(test_dataset_dog), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "    print('image :: ', inputs.shape)\n",
    "    print(targets)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FoodX-251 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = \"local\" # comment out if using ds from the shared folder\n",
    "#ds_type = \"shared\" # comment out if using ds from the local folder\n",
    "\n",
    "\n",
    "if (ds_type == \"local\"):\n",
    "    data_dir = \"/home/dmitry.demidov/Documents/Datasets/FoodX-251\"\n",
    "\n",
    "    split = 'train'\n",
    "    train_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    train_df['path'] = train_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}/{split}_set/', x))\n",
    "\n",
    "    split = 'val'\n",
    "    test_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    test_df['path'] = test_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}/{split}_set/', x))\n",
    "\n",
    "elif (ds_type == \"shared\"):\n",
    "    data_dir = \"/apps/local/shared/CV703/datasets/FoodX/food_dataset\"\n",
    "\n",
    "    split = 'train'\n",
    "    train_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    train_df['path'] = train_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}_set/', x))\n",
    "\n",
    "    split = 'val'\n",
    "    test_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    test_df['path'] = test_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}_set/', x))\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Choose dataset type (local/shared)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 118475\n",
      "Number of test samples: 11994\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FOODDataset(train_df)\n",
    "test_dataset = FOODDataset(test_df)\n",
    "print('Number of train samples:', len(train_dataset))\n",
    "print('Number of test samples:', len(test_dataset))\n",
    "\n",
    "# load in into the torch dataloader to get variable batch size, shuffle \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, drop_last=True, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, drop_last=False, shuffle=True) # Not enough memory for more than 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118475 11994\n",
      "462 375\n",
      "torch.Size([256, 3, 224, 224])\n",
      "tensor([100,  60,  87, 152,  25,  95, 179, 210, 187, 165,  65,  69, 178,   4,\n",
      "        218, 207, 128,  55, 250,  94, 246,  44,  64, 146,  85,  13,  17, 215,\n",
      "         81,   9,  39,  91,  28,  17, 148, 113, 232, 151, 128, 116,  58, 166,\n",
      "        176,  94, 247,  79, 157, 129,  25, 147,  39, 237,   7, 113,  21, 155,\n",
      "        230, 168,  70, 190, 214,  21, 160,  35, 150, 106, 124, 194, 152, 228,\n",
      "        136, 198,  94,  30, 214, 249, 190,  31,  98, 221,  19, 115, 139,  74,\n",
      "        142,  13, 138, 177, 178,  37, 156,  74, 122, 105,   6, 132, 205,  11,\n",
      "        212, 245,  89, 217,  18, 212, 226,   3,  74,  68, 123, 171, 181, 152,\n",
      "        249,  58, 143, 247, 202, 113,  39, 243,  55,  40, 147,  88,  12,  38,\n",
      "        250,  30, 220,  89,  50, 146, 107, 139, 163, 134,  42,  37,  93, 123,\n",
      "         13,  39, 148,  95, 197,  70,   0,  56,   8,  84, 201, 131,  98, 161,\n",
      "          0, 196, 132, 244, 227, 143, 172,  35, 176, 133, 142, 241, 163,  94,\n",
      "         99, 228,  93, 198, 189,  42, 151, 122, 150,   0,  79, 220, 145, 117,\n",
      "        240, 235, 171, 164, 111,  59, 250, 142,  68,  64, 209,   4,  55, 118,\n",
      "         89, 123, 177, 102, 144, 108,   1, 116,  25,  20, 193, 169, 121, 238,\n",
      "         84,  24, 221, 151,  79, 135, 160, 203,  19,  37,  35,  85,  32,  22,\n",
      "         95,   2, 133, 150, 146, 235, 133, 112,  45, 221, 202,  71, 133, 226,\n",
      "        232, 196, 246, 231,  26, 128,  97, 136, 177,  50,   6, 140,  63,  56,\n",
      "        123, 110,  37, 102])\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics about the dataset:\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "\n",
    "print(len(train_loader), len(test_loader))\n",
    "\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(inputs.shape)\n",
    "    print(labels)\n",
    "    print('='*50)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare ViT for transfer learning (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # we will use only the last class token (produced by the last block) for transfer learning\n",
    "# model = deit_base_patch16_224(pretrained=True, use_top_n_heads=8,use_patch_outputs=False).cuda()\n",
    "\n",
    "# # freeze backbone and add linear classifier on top that\n",
    "# # for param in model.parameters():\n",
    "# #     param.requires_grad = True # False\n",
    "# model.head = torch.nn.Linear(in_features=model.head.in_features, out_features=classes_number)\n",
    "\n",
    "# model.head.apply(model._init_weights)\n",
    "# # for param in model.head.parameters():\n",
    "# #     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.003, betas=(0.5, 0.999))\n",
    "\n",
    "# #model.train()\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 100\n",
    "# print('Training....')\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     epoch_loss = 0.0\n",
    "#     epoch_acc = 0.0\n",
    "#     #running_loss = 0.0\n",
    "\n",
    "#     with tqdm(train_loader) as p_bar:\n",
    "#         for samples, targets in p_bar:\n",
    "#             samples = samples.to(device)\n",
    "#             targets = targets.to(device)\n",
    "            \n",
    "#             outputs = model(samples) #, fine_tune=False)\n",
    "#             loss = criterion(outputs, targets)\n",
    "\n",
    "#             loss_value = loss.item()\n",
    "#             if not math.isfinite(loss_value):\n",
    "#                 print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "#                 sys.exit(1)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             epoch_loss += outputs.shape[0] * loss.item()\n",
    "\n",
    "#             epoch_acc += torch.sum(outputs.argmax(dim=-1) == targets).item()\n",
    "\n",
    "#             # print statistics\n",
    "#             #running_loss += loss.item()\n",
    "\n",
    "#     # if i % 100 == 99:    # print every 10 mini-batches\n",
    "#     #     print('[%d, %5d] loss: %.3f' %\n",
    "#     # (epoch + 1, i + 1, running_loss / 100))\n",
    "#     # running_loss = 0.0\n",
    "\n",
    "\n",
    "#     loss = epoch_loss / len(train_dataset)\n",
    "#     acc = epoch_acc / len(train_dataset)\n",
    "\n",
    "\n",
    "#     # print statistics\n",
    "#     print(\"Epoch:\", epoch+1, \"|\", \"Loss:\", loss,\n",
    "#         'Instant Accuracy:{0:.3%}'.format(acc))\n",
    "\n",
    "#     f = open(\"./models/statistics.txt\", \"a\")\n",
    "#     f.write(\"Epoch: \" + str(epoch) + \", \" +\"Loss: \"+ str(loss) + \", \"+\"Accuracy: \"+ str(acc) + \"\\n\")\n",
    "#     f.close()\n",
    "    \n",
    "#     torch.save({'state_dict': model.state_dict()}, './models/model_{0}ep_{1:.1}loss_{2:.3}acc.pt'.format(epoch, loss, acc,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training + Evaluation (in parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test funcs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, criterion, optimizer, train_dataset, train_loader, epoch):\n",
    "    print('Training....')\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    acc=0.0\n",
    "\n",
    "    with tqdm(train_loader) as p_bar:\n",
    "        for samples, targets in p_bar:\n",
    "            samples = samples.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(samples, fine_tune=True)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss_value = loss.item()\n",
    "            if not math.isfinite(loss_value):\n",
    "                print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "                sys.exit(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += outputs.shape[0] * loss.item()\n",
    "\n",
    "            acc+=torch.sum(outputs.argmax(dim=-1) == targets).item()\n",
    "\n",
    "    loss_print = epoch_loss / len(train_dataset)\n",
    "    acc_print = acc/len(train_dataset)\n",
    "    epoch_print = epoch+1\n",
    "\n",
    "    print(\"Epoch:\", epoch_print, \"|\", \"Loss:\", loss_print)\n",
    "    print(\"Train Accuracy:{0:.3%}\".format(acc_print))\n",
    "\n",
    "    f = open(\"./models/statistics.txt\", \"a\")\n",
    "    text_train = \"Epoch: \" + str(epoch_print) + \", \" + \"Train Loss: \" + str(loss_print) + \", \" + \"Train Accuracy: \" + str(acc_print) + \"\\n\"\n",
    "    f.write(text_train)\n",
    "    f.close()\n",
    "    \n",
    "    torch.save({'state_dict': model.state_dict()}, './models/model_{0}ep_{1:.2}loss.pt'.format(epoch_print, loss_print))\n",
    "\n",
    "    sent_results(text=text_train)\n",
    "\n",
    "    #del samples\n",
    "    #del targets\n",
    "\n",
    "\n",
    "\n",
    "def test(device, model, criterion, test_dataset, test_loader, model_path = './folder/path.pt', test_only = False):\n",
    "    print('Testing....')\n",
    "\n",
    "    if test_only:\n",
    "        print('Test only!')\n",
    "        state_dict = torch.load(model_path)['state_dict']\n",
    "        model.load_state_dict(state_dict)\n",
    "        model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    acc=0.0\n",
    "\n",
    "    with tqdm(test_loader) as p_bar:\n",
    "        for samples, targets in p_bar:\n",
    "            samples = samples.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(samples, fine_tune=False)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            epoch_loss += outputs.shape[0] * loss.item()\n",
    "\n",
    "            acc+=torch.sum(outputs.argmax(dim=-1) == targets).item()\n",
    "\n",
    "    acc_print = acc/len(test_dataset)\n",
    "    loss_print = epoch_loss / len(test_dataset)\n",
    "\n",
    "    print(\"Test Loss:\", loss_print)\n",
    "    print('Test Accuracy:{0:.3%}'.format(acc_print))\n",
    "\n",
    "    f = open(\"./models/statistics.txt\", \"a\")\n",
    "    text_test = \"Test Loss: \"+ str(loss_print) + \", \" + \"Test Accuracy: \" + str(acc_print) + \"\\n\"\n",
    "    f.write(text_test)\n",
    "    f.close()\n",
    "\n",
    "    sent_results(text=text_test)\n",
    "\n",
    "    #del samples\n",
    "    #del targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sent results to an email (optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Email sent successfully!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import sent_results\n",
    "\n",
    "# Test:\n",
    "sent_results(text='Start')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=6144, out_features=251, bias=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = deit_base_patch16_224(pretrained=True, use_top_n_heads=8,use_patch_outputs=False).cuda()\n",
    "#model = deit_small_patch16_224(pretrained=True, use_top_n_heads=8,use_patch_outputs=False).cuda()\n",
    "\n",
    "# # Freeze backbone:\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True # original: False\n",
    "\n",
    "# Add linear classifier on top:\n",
    "model.head = torch.nn.Linear(in_features=model.head.in_features, out_features=classes_number)\n",
    "model.head.apply(model._init_weights)\n",
    "# for param in model.head.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training + validation in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [42:13<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 2.602127376442897\n",
      "Train Accuracy:42.697%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:32<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8005262642318614\n",
      "Test Accuracy:55.544%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [16:37<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 1.8689761778200997\n",
      "Train Accuracy:55.924%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7173048071171892\n",
      "Test Accuracy:57.579%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:31<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 1.6101517504764322\n",
      "Train Accuracy:60.988%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.744987237169203\n",
      "Test Accuracy:57.212%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:33<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss: 1.433250160837103\n",
      "Train Accuracy:64.496%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:23<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7027152762405868\n",
      "Test Accuracy:58.463%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:31<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Loss: 1.3008355920247876\n",
      "Train Accuracy:67.339%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:23<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7374989859994618\n",
      "Test Accuracy:58.479%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:30<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Loss: 1.1942556651289087\n",
      "Train Accuracy:69.594%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7816522898267702\n",
      "Test Accuracy:58.354%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:33<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Loss: 1.101364097748157\n",
      "Train Accuracy:71.672%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8265115668898089\n",
      "Test Accuracy:57.395%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:31<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Loss: 1.022652167380422\n",
      "Train Accuracy:73.396%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:23<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7827207471840536\n",
      "Test Accuracy:58.196%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:30<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Loss: 0.9551903280695674\n",
      "Train Accuracy:74.769%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8510074172593245\n",
      "Test Accuracy:57.295%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:40<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 0.8940659377514045\n",
      "Train Accuracy:76.327%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.9153156694181963\n",
      "Test Accuracy:56.962%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:38<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Loss: 0.83789739592263\n",
      "Train Accuracy:77.691%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8830689602103337\n",
      "Test Accuracy:57.829%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:26<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Loss: 0.7839818278544309\n",
      "Train Accuracy:79.065%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.954761978823363\n",
      "Test Accuracy:56.928%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:25<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Loss: 0.7362238494171123\n",
      "Train Accuracy:80.280%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:19<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.970781674519447\n",
      "Test Accuracy:57.520%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:27<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Loss: 0.6952945371950998\n",
      "Train Accuracy:81.304%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.9945529741864811\n",
      "Test Accuracy:56.670%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:31<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Loss: 0.6554656086664307\n",
      "Train Accuracy:82.232%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.008374331771522\n",
      "Test Accuracy:56.895%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:30<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Loss: 0.6191934487446875\n",
      "Train Accuracy:83.117%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.0412181505903275\n",
      "Test Accuracy:56.837%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:35<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Loss: 0.5853451644984397\n",
      "Train Accuracy:84.175%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.0808106353924196\n",
      "Test Accuracy:56.670%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:34<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Loss: 0.5576447219027575\n",
      "Train Accuracy:84.712%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.0983206890256323\n",
      "Test Accuracy:56.978%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:29<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Loss: 0.5209996763938941\n",
      "Train Accuracy:85.834%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:20<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.127342182098676\n",
      "Test Accuracy:56.537%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:25<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Loss: 0.4986789809140658\n",
      "Train Accuracy:86.390%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.144308435334471\n",
      "Test Accuracy:56.286%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:29<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Loss: 0.47348117491091624\n",
      "Train Accuracy:87.008%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.1851335905861293\n",
      "Test Accuracy:56.345%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:32<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Loss: 0.44692795968604104\n",
      "Train Accuracy:87.819%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.18681697593404\n",
      "Test Accuracy:56.495%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:31<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Loss: 0.4275366283705728\n",
      "Train Accuracy:88.329%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.247680110336642\n",
      "Test Accuracy:55.761%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:21<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Loss: 0.4015784313478005\n",
      "Train Accuracy:89.214%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.245173689323961\n",
      "Test Accuracy:56.495%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:22<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Loss: 0.38088590877522494\n",
      "Train Accuracy:89.826%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:20<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3162277274794114\n",
      "Test Accuracy:55.611%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:33<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Loss: 0.3643084753180989\n",
      "Train Accuracy:90.282%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3012560301270706\n",
      "Test Accuracy:56.020%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:38<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Loss: 0.34492365286304466\n",
      "Train Accuracy:90.701%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:22<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3399747711271965\n",
      "Test Accuracy:55.786%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:36<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Loss: 0.32351259169183116\n",
      "Train Accuracy:91.526%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.373192827284525\n",
      "Test Accuracy:55.211%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:36<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Loss: 0.31325340758800807\n",
      "Train Accuracy:91.733%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:21<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.399460998320154\n",
      "Test Accuracy:55.386%\n",
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [15:36<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Loss: 0.29862291540921454\n",
      "Train Accuracy:92.140%\n",
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:23<00:00,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.429801371210075\n",
      "Test Accuracy:55.278%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train (device=device, model=model, criterion=criterion, optimizer=optimizer, \n",
    "        train_dataset=train_dataset, train_loader=train_loader, epoch=epoch)\n",
    "\n",
    "    test (device=device, model=model, criterion=criterion, \n",
    "        test_dataset=test_dataset, test_loader=test_loader, test_only = False) #, model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test only (optional):\n",
    "To check a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing....\n",
      "Test only!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:26<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7027152970530142\n",
      "Test Accuracy:58.463%\n"
     ]
    }
   ],
   "source": [
    "# It is assumed that a model is defined already\n",
    "model_path = './models/model_3ep_1.4loss.pt'\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "test (device=device, model=model, criterion=criterion, \n",
    "    test_dataset=test_dataset, test_loader=test_loader, test_only = True, model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FocalLoss(torch.nn.Module):\n",
    "#     def __init__(self, alpha=1, gamma=2, reduce=True):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.alpha = alpha\n",
    "#         self.gamma = gamma\n",
    "#         self.reduce = reduce\n",
    "\n",
    "#     def forward(self, inputs, targets):\n",
    "#         BCE_loss = torch.nn.CrossEntropyLoss()(inputs, targets)\n",
    "\n",
    "#         pt = torch.exp(-BCE_loss)\n",
    "#         F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "#         if self.reduce:\n",
    "#             return torch.mean(F_loss)\n",
    "#         else:\n",
    "#             return F_loss\n",
    "\n",
    "# criterion = FocalLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms.CenterCrop(224) # instead of Resize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List\n",
    "* Change number of training epochs\n",
    "* Change number of class tokens e.g, use_top_n_heads=4, etc\n",
    "* New:\n",
    "* - Try focal loss (from Rushali)\n",
    "* - Try some transformations (Flipping, Rotation, CenterCrop ??)\n",
    "* - Combine resnet and DeiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1a7d7175cab92eb6dbde117c0cc0b7f8055ecb32409ac704c7d8562db6b2b75"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('cv703': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}