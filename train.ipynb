{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV-703, Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from models_to_finetune import deit_small_patch16_224\n",
    "\n",
    "from datasets import CUBDataset, DOGDataset, FOODDataset\n",
    "\n",
    "\n",
    "\n",
    "# from __future__ import print_function, division\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.optim import lr_scheduler\n",
    "\n",
    "# import torchvision\n",
    "# from torchvision import datasets, models, transforms\n",
    "# import torchvision.transforms as T\n",
    "\n",
    "# from PIL import Image\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# import scipy.io #for dogs dateset\n",
    "\n",
    "# import time\n",
    "# import os\n",
    "# import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup a dataset\n",
    "Uncomment that one dataset you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUB-200-2011 (Birds) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 5994\n",
      "Number of test samples: 5794\n"
     ]
    }
   ],
   "source": [
    "data_root = \"/apps/local/shared/CV703/datasets/CUB/CUB_200_2011/\"\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "# Write data transform here as per the requirement\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "train_dataset = CUBDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"train\")\n",
    "test_dataset = CUBDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset))\n",
    "print('Number of test samples:', len(test_dataset))\n",
    "\n",
    "\n",
    "# Load in into the torch dataloader to get variable batch size, shuffle \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, drop_last=True, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, drop_last=False, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187, 182)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "tensor([ 85,  49, 163,  26, 170, 104, 171,  13, 160,  60,  50, 166, 121,  46,\n",
      "        144, 106, 102,   6, 107, 141,  28,   9,  77,  17,  58, 108, 187, 190,\n",
      "         80,  64,  88, 144])\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(inputs.shape)\n",
    "    print(labels)\n",
    "    print('='*50)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Dogs Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 12000\n",
      "Number of test samples: 8580\n"
     ]
    }
   ],
   "source": [
    "data_root = \"/apps/local/shared/CV703/datasets/dog/\"\n",
    "\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "\n",
    "train_dataset = DOGDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"train\")\n",
    "test_dataset = DOGDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset))\n",
    "print('Number of test samples:', len(test_dataset))\n",
    "\n",
    "# Load in into the torch dataloader to get variable batch size, shuffle \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, drop_last=True, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, drop_last=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 269)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "tensor([ 96,  24,   3,  86,  13,  40,  34, 105, 108,  80,  46,  43,  27,  92,\n",
      "        111,  45,  36,  80,  63, 105,  73,  90,  45,   9,  40,  32,  44,  31,\n",
      "         23,  99,  32,  57])\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    print(inputs.shape)\n",
    "    print(labels)\n",
    "    print('='*50)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUB-200-2011 + Stanford Dog (concatenated) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 5994\n",
      "Number of test samples: 5794\n"
     ]
    }
   ],
   "source": [
    "# CUB:\n",
    "data_root_bird = \"/apps/local/shared/CV703/datasets/CUB/CUB_200_2011/\"\n",
    "\n",
    "mean_bird = (0.485, 0.456, 0.406)\n",
    "std_bird = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "# write data transform here as per the requirement\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_bird, std=std_bird)\n",
    "    ])\n",
    "\n",
    "train_dataset_cub = CUBDataset(image_root_path=f\"{data_root_bird}\", transform=data_transform, split=\"train\")\n",
    "test_dataset_cub = CUBDataset(image_root_path=f\"{data_root_bird}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset_cub))\n",
    "print('Number of test samples:', len(test_dataset_cub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 12000\n",
      "Number of test samples: 8580\n"
     ]
    }
   ],
   "source": [
    "# Dog:\n",
    "mean_dog = (0.485, 0.456, 0.406)\n",
    "std_dog = (0.229, 0.224, 0.225)\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_dog, std=std_dog)\n",
    "    ])\n",
    "\n",
    "\n",
    "data_root_dog = \"/apps/local/shared/CV703/datasets/dog/\"\n",
    "\n",
    "# TODO: Start labels counting from 200 (in case of concatenation only)!\n",
    "train_dataset_dog = DOGDataset(image_root_path=f\"{data_root_dog}\", transform=data_transform, split=\"train\")\n",
    "test_dataset_dog = DOGDataset(image_root_path=f\"{data_root_dog}\", transform=data_transform, split=\"test\")\n",
    "print('Number of train samples:', len(train_dataset_dog))\n",
    "print('Number of test samples:', len(test_dataset_dog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated dataloader for CUB and DOG\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "             torch.utils.data.ConcatDataset([train_dataset_cub, train_dataset_dog]),\n",
    "             batch_size=32, shuffle=True,\n",
    "             num_workers=1, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "             torch.utils.data.ConcatDataset([test_dataset_cub, test_dataset_dog]),\n",
    "             batch_size=32, shuffle=True,\n",
    "             num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5994, 12000, 17994)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset_cub), len(train_dataset_dog), len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5794, 8580, 14374)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset_cub), len(test_dataset_dog), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image ::  torch.Size([1, 3, 224, 224])\n",
      "tensor([35])\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "    print('image :: ', inputs.shape)\n",
    "    print(targets)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FoodX-251 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_type = \"local\" # comment out if using ds from the shared folder\n",
    "ds_type = \"shared\" # comment out if using ds from the local folder\n",
    "\n",
    "\n",
    "if (ds_type == \"local\"):\n",
    "    data_dir = \"/home/u20020067/Documents/Datasets/FoodX-251\"\n",
    "\n",
    "    split = 'train'\n",
    "    train_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    train_df['path'] = train_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}/{split}_set/', x))\n",
    "\n",
    "    split = 'val'\n",
    "    test_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    test_df['path'] = test_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}/{split}_set/', x))\n",
    "\n",
    "elif (ds_type == \"shared\"):\n",
    "    data_dir = \"/apps/local/shared/CV703/datasets/FoodX/food_dataset\"\n",
    "\n",
    "    split = 'train'\n",
    "    train_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    train_df['path'] = train_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}_set/', x))\n",
    "\n",
    "    split = 'val'\n",
    "    test_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "    test_df['path'] = test_df['image_name'].map(lambda x: os.path.join(f'{data_dir}/{split}_set/', x))\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Choose dataset type (local/shared)!\")\n",
    "\n",
    "\n",
    "#test_df['path'] = test_df['image_name'].map(lambda x: os.path.join(f'./{split}/{split}_set/', x)) # original\n",
    "\n",
    "\n",
    "# train_dir = '/home/u20020067/Documents/Datasets/FoodX-251/train/train_set/'\n",
    "# val_dir = '/home/u20020067/Documents/Datasets/FoodX-251/val/val_set/'\n",
    "\n",
    "# train_df = pd.read_csv('/home/u20020067/Documents/Datasets/FoodX-251/annot/train_info.csv', names= ['img_name','label'])\n",
    "# train_df['path'] = train_df['img_name'].map(lambda x: os.path.join(train_dir,x))\n",
    "# val_df = pd.read_csv('/home/u20020067/Documents/Datasets/FoodX-251/annot/val_info.csv', names= ['img_name','label'])\n",
    "# val_df['path'] = val_df['img_name'].map(lambda x: os.path.join(val_dir,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 118475\n",
      "Number of test samples: 11994\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FOODDataset(train_df)\n",
    "test_dataset = FOODDataset(test_df)\n",
    "print('Number of train samples:', len(train_dataset))\n",
    "print('Number of test samples:', len(test_dataset))\n",
    "\n",
    "# load in into the torch dataloader to get variable batch size, shuffle \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, drop_last=True, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, drop_last=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118475, 11994)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3702, 375)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "tensor([143,  33, 205, 203, 119,  50, 242, 233,  38, 107,  59,  80, 105, 183,\n",
      "         39, 109, 124,   9,  36,  94,   3, 240, 144, 161,  57,  33, 138, 136,\n",
      "        134, 224, 227, 217])\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(inputs.shape)\n",
    "    print(labels)\n",
    "    print('='*50)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FoodX (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Set train and test set\n",
    "# mean = (0.485, 0.456, 0.406)\n",
    "# std = (0.229, 0.224, 0.225)\n",
    "# data_transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=mean, std=std)\n",
    "#     ])\n",
    "# data_root = \"./dog/\"\n",
    "# train_dataset = DOGDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"train\")\n",
    "# test_dataset = DOGDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"test\")\n",
    "# print('Number of train samples:', len(train_dataset))\n",
    "# print('Number of test samples:', len(test_dataset))\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, drop_last=True, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, drop_last=False)\n",
    "\n",
    "\n",
    "\n",
    "#data_root = \"/home/u20020067/Documents/Datasets/CUB_200_2011/CUB_200_2011/\"\n",
    "\n",
    "# mean = (0.485, 0.456, 0.406)\n",
    "# std = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "# # write data transform here as per the requirement\n",
    "# data_transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=mean, std=std)\n",
    "#     ])\n",
    "\n",
    "# train_dataset = CUBDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"train\")\n",
    "# test_dataset = CUBDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"test\")\n",
    "\n",
    "\n",
    "# # load in into the torch dataloader to get variable batch size, shuffle \n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, drop_last=True, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, drop_last=False, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_dir = \"/apps/local/shared/CV703/datasets/FoodX/food_dataset/\"\n",
    "\n",
    "# split = 'train'\n",
    "# train_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "# train_df['path'] = train_df['image_name'].map(lambda x: os.path.join(f'./{split}/{split}_set/', x))\n",
    "\n",
    "\n",
    "# split = 'val'\n",
    "# val_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "# val_df['path'] = val_df['image_name'].map(lambda x: os.path.join(f'./{split}/{split}_set/', x))\n",
    "\n",
    "\n",
    "# train_dataset = FOODDataset(train_df)\n",
    "# val_dataset = FOODDataset(val_df)\n",
    "\n",
    "# # load in into the torch dataloader to get variable batch size, shuffle \n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, drop_last=True, shuffle=True)\n",
    "# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, drop_last=False, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #data_dir = \"/home/u20020067/Documents/Datasets/FoodX/food_dataset\"\n",
    "# data_dir = \"/home/u20020067/Documents/Datasets/FoodX-251\"\n",
    "\n",
    "# split = 'train'\n",
    "# train_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "# train_df['path'] = train_df['image_name'].map(lambda x: os.path.join(f'./{split}/{split}_set/', x))\n",
    "\n",
    "\n",
    "# split = 'val'\n",
    "# val_df = pd.read_csv(f'{data_dir}/annot/{split}_info.csv', names= ['image_name','label'])\n",
    "# val_df['path'] = val_df['image_name'].map(lambda x: os.path.join(f'./{split}/{split}_set/', x))\n",
    "\n",
    "\n",
    "# # train_dir = '/home/u20020067/Documents/Datasets/FoodX-251/train/train_set/'\n",
    "# # val_dir = '/home/u20020067/Documents/Datasets/FoodX-251/val/val_set/'\n",
    "\n",
    "# # train_df = pd.read_csv('/home/u20020067/Documents/Datasets/FoodX-251/annot/train_info.csv', names= ['img_name','label'])\n",
    "# # train_df['path'] = train_df['img_name'].map(lambda x: os.path.join(train_dir,x))\n",
    "# # val_df = pd.read_csv('/home/u20020067/Documents/Datasets/FoodX-251/annot/val_info.csv', names= ['img_name','label'])\n",
    "# # val_df['path'] = val_df['img_name'].map(lambda x: os.path.join(val_dir,x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = FOODDataset(train_df)\n",
    "# val_dataset = FOODDataset(val_df)\n",
    "\n",
    "\n",
    "# # load in into the torch dataloader to get variable batch size, shuffle \n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, drop_last=True, shuffle=True)\n",
    "# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, drop_last=False, shuffle=True)\n",
    "\n",
    "# len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare ViT for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "# we will use only the last class token (produced by the last block) for transfer learning\n",
    "model = deit_small_patch16_224(pretrained=True, use_top_n_heads=4,use_patch_outputs=False).cuda()\n",
    "\n",
    "# freeze backbone and add linear classifier on top that\n",
    "for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "model.head = torch.nn.Linear(in_features=model.head.in_features, out_features=classes_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.head.apply(model._init_weights)\n",
    "for param in model.head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3702/3702 [08:42<00:00,  7.08it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "print('Training....')\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(train_loader) as p_bar:\n",
    "        for samples, targets in p_bar:\n",
    "            samples = samples.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(samples, fine_tune=True)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss_value = loss.item()\n",
    "            if not math.isfinite(loss_value):\n",
    "                print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "                sys.exit(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:52<00:00,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:37.077%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing....')\n",
    "acc=0\n",
    "with tqdm(test_loader) as p_bar:\n",
    "    for samples, targets in p_bar:\n",
    "        samples = samples.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        outputs = model(samples, fine_tune=True)\n",
    "        acc+=torch.sum(outputs.argmax(dim=-1) == targets).item()\n",
    "\n",
    "print('Accuracy:{0:.3%}'.format(acc/len(test_dataset)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "* Change number of training epochs\n",
    "* Change number of class tokens e.g, use_top_n_heads=4, etc"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ec74bd9cb25c7f3d818fc24baee49db9b312a136e1d154714bc57dce796a060"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('cv703': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}